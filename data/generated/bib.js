const generatedBibEntries = {
    "hu_chatdb_2023": {
        "abstract": "Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, influenced by biological brains. We propose a symbolic memory framework for LLMs, instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the databases. This framework supports complex multi-hop reasoning and is validated on a synthetic dataset.",
        "author": "Hu, Chenxu and Fu, Jie and Du, Chenzhuang and Luo, Simian and Zhao, Junbo and Zhao, Hang",
        "date": "2023-06-07",
        "doi": "10.48550/arXiv.2306.03901",
        "eprint": "2306.03901 [cs]",
        "eprinttype": "arxiv",
        "file": "Hu et al. - 2023 - ChatDB Augmenting LLMs with Databases as Their Symbolic Memory.pdf:/Users/endofstream/Zotero/storage/RNXTES3D/Hu et al. - 2023 - ChatDB Augmenting LLMs with Databases as Their Symbolic Memory.pdf:application/pdf",
        "keywords": "evaluation:quantitative, supported_operations:insert, supported_operations:delete, supported_operations:update, retrieval_data_type:text, retrieval_method:LLM-generated_SQL_query, updating_method:LLM-generated_SQL_query, LLMs, symbolic memory, SQL databases, multi-hop reasoning, memory framework",
        "langid": "english",
        "number": "arXiv:2306.03901",
        "publisher": "arXiv",
        "shorttitle": "ChatDB",
        "title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
        "type": "misc",
        "url": "http://arxiv.org/abs/2306.03901",
        "urldate": "2024-05-14",
        "year": "2023"
    },
    "liu_think--memory_2023": {
        "abstract": "Memory-augmented LLMs have demonstrated remarkable performance in long-term human-machine interactions, relying on iterative recalling and reasoning of history. However, repeated recall-reason steps can produce biased thoughts. We propose TiM (Think-in-Memory), a novel memory mechanism that enables LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream. TiM consists of two stages: recalling relevant thoughts from memory before generating a response, and post-thinking to incorporate both historical and new thoughts to update the memory. This framework eliminates repeated reasoning by saving post-thinking thoughts as history.",
        "author": "Liu, Lei and Yang, Xiaoyan and Shen, Yue and Hu, Binbin and Zhang, Zhiqiang and Gu, Jinjie and Zhang, Guannan",
        "date": "2023-11-15",
        "doi": "10.48550/arXiv.2311.08719",
        "eprint": "2311.08719 [cs]",
        "eprinttype": "arxiv",
        "file": "Liu et al. - 2023 - Think-in-Memory Recalling and Post-thinking Enable LLMs with Long-Term Memory.pdf:/Users/endofstream/Zotero/storage/NDYKUEC9/Liu et al. - 2023 - Think-in-Memory Recalling and Post-thinking Enable LLMs with Long-Term Memory.pdf:application/pdf",
        "keywords": "evaluation:quantitative, evaluation:qualitative, supported_operations:insert, supported_operations:delete, supported_operations:update, retrieval_data_type:text, retrieval_method:vector_similarity_search, updating_method:LLM-generated_sentences, LLMs, long-term memory, iterative recalling, post-thinking, conversation stream",
        "langid": "english",
        "number": "arXiv:2311.08719",
        "publisher": "arXiv",
        "shorttitle": "Think-in-Memory",
        "title": "Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory",
        "type": "misc",
        "url": "http://arxiv.org/abs/2311.08719",
        "urldate": "2024-03-30",
        "year": "2023"
    },
    "lu_memochat_2023": {
        "abstract": "We propose MemoChat, a pipeline for refining instructions that enables large language models (LLMs) to effectively employ self-composed memos for maintaining consistent long-range open-domain conversations. We demonstrate a long-range open-domain conversation through iterative \u201cmemorization-retrieval-response\u201d cycles. This requires us to carefully design tailored tuning instructions for each distinct stage. The instructions are reconstructed from a collection of public datasets to teach the LLMs to memorize and retrieve past dialogues with structured memos, leading to enhanced consistency when participating in future conversations.",
        "author": "Lu, Junru and An, Siyu and Lin, Mingbao and Pergola, Gabriele and He, Yulan and Yin, Di and Sun, Xing and Wu, Yunsheng",
        "date": "2023-08-22",
        "doi": "10.48550/arXiv.2308.08239",
        "eprint": "2308.08239 [cs]",
        "eprinttype": "arxiv",
        "file": "Lu et al. - 2023 - MemoChat Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation.pdf:/Users/endofstream/Zotero/storage/EEJ6N3VN/Lu et al. - 2023 - MemoChat Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation.pdf:application/pdf",
        "keywords": "evaluation:quantitative, supported_operations:insert, supported_operations:delete, supported_operations:update, retrieval_data_type:text, retrieval_method:vector_similarity_search, updating_method:LLM-generated_sentences, LLMs, memos, long-range conversation, open-domain conversation, memorization-retrieval-response",
        "langid": "english",
        "number": "arXiv:2308.08239",
        "publisher": "arXiv",
        "shorttitle": "MemoChat",
        "title": "MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation",
        "type": "misc",
        "url": "http://arxiv.org/abs/2308.08239",
        "urldate": "2024-05-15",
        "year": "2023"
    },
    "modarressi_ret-llm_2023": {
        "abstract": "LLMs have significantly advanced NLP through extensive parameters and data utilization. However, existing LLMs lack a dedicated memory unit for explicit knowledge storage and retrieval. We propose RET-LLM, a framework that equips LLMs with a general write-read memory unit, extracting and saving knowledge as triplets. This scalable, aggregatable, updatable, and interpretable memory unit demonstrates superior performance in question-answering tasks and handling time-dependent information.",
        "author": "Modarressi, Ali and Imani, Ayyoob and Fayyaz, Mohsen and Sch\u00fctze, Hinrich",
        "date": "2023-05-23",
        "doi": "10.48550/arXiv.2305.14322",
        "eprint": "2305.14322 [cs]",
        "eprinttype": "arxiv",
        "file": "Modarressi et al. - 2023 - RET-LLM Towards a General Read-Write Memory for Large Language Models.pdf:/Users/endofstream/Zotero/storage/GPW3NYBU/Modarressi et al. - 2023 - RET-LLM Towards a General Read-Write Memory for Large Language Models.pdf:application/pdf",
        "keywords": "evaluation:qualitative, supported_operations:insert, retrieval_data_type:Triplet, retrieval_method:exact_match, retrieval_method:vector_similarity_search, updating_method:LLM-generated_Triplet_query, LLMs, memory unit, knowledge storage, retrieval, NLP, question-answering, temporal information",
        "langid": "english",
        "number": "arXiv:2305.14322",
        "publisher": "arXiv",
        "shorttitle": "RET-LLM",
        "title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
        "type": "misc",
        "url": "http://arxiv.org/abs/2305.14322",
        "urldate": "2024-05-14",
        "year": "2023"
    },
    "packer_memgpt_2024": {
        "abstract": "Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems which provide the illusion of an extended virtual memory via paging between physical memory and disk. Using this technique, we introduce MemGPT (MemoryGPT), a system that intelligently manages different storage tiers in order to effectively provide extended context within the LLM\u2019s limited context window. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicap their performance: document analysis and multi-session chat. We release MemGPT code and data for our experiments at https://research.memgpt.ai.",
        "author": "Packer, Charles and Wooders, Sarah and Lin, Kevin and Fang, Vivian and Patil, Shishir G. and Stoica, Ion and Gonzalez, Joseph E.",
        "date": "2024-02-12",
        "doi": "10.48550/arXiv.2310.08560",
        "eprint": "2310.08560 [cs]",
        "eprinttype": "arxiv",
        "file": "Packer et al. - 2024 - MemGPT Towards LLMs as Operating Systems.pdf:/Users/endofstream/Zotero/storage/8S58MQMW/Packer et al. - 2024 - MemGPT Towards LLMs as Operating Systems.pdf:application/pdf",
        "keywords": "evaluation:quantitative, evaluation:qualitative, supported_operations:insert, supported_operations:delete, supported_operations:update, retrieval_data_type:text, retrieval_method:vector_similarity_search, retrieval_method:KV_search, updating_method:LLM-generated_KV_pairs, updating_method:LLM-generated_sentences, LLMs, memory, context management, AI, document analysis, multi-session chat",
        "langid": "english",
        "number": "arXiv:2310.08560",
        "publisher": "arXiv",
        "shorttitle": "MemGPT",
        "title": "MemGPT: Towards LLMs as Operating Systems",
        "type": "misc",
        "url": "http://arxiv.org/abs/2310.08560",
        "urldate": "2024-03-16",
        "year": "2024"
    },
    "park_generative_2023": {
        "author": "Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.",
        "booktitle": "Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology",
        "date": "2023-10-29",
        "doi": "10.1145/3586183.3606763",
        "eventtitle": "UIST '23: The 36th Annual ACM Symposium on User Interface Software and Technology",
        "file": "Park et al. - 2023 - Generative Agents Interactive Simulacra of Human Behavior.pdf:/Users/endofstream/Zotero/storage/TRXAYYW5/Park et al. - 2023 - Generative Agents Interactive Simulacra of Human Behavior.pdf:application/pdf",
        "isbn": "9798400701320",
        "keywords": "evaluation:quantitative, evaluation:qualitative, evaluation:controlled_user_study, supported_operations:insert, retrieval_data_type:text, retrieval_method:vector_similarity_search, updating_method:LLM-generated_sentences, LLMs, interactive simulacra, human behavior, generative agents, user interface",
        "langid": "english",
        "location": "San Francisco, CA, USA",
        "pages": "1--22",
        "publisher": "ACM",
        "shorttitle": "Generative Agents",
        "title": "Generative Agents: Interactive Simulacra of Human Behavior",
        "type": "inproceedings",
        "url": "https://dl.acm.org/doi/10.1145/3586183.3606763",
        "urldate": "2024-05-15",
        "year": "2023"
    },
    "schuurmans_memory_2023": {
        "abstract": "We show that transformer-based large language models are computationally universal when augmented with an external memory. Any deterministic language model that conditions on strings of bounded length is equivalent to a finite automaton, hence computationally limited. However, augmenting such models with a read-write memory creates the possibility of processing arbitrarily large inputs and simulating any algorithm. We establish that an existing large language model, Flan-U-PaLM 540B, can simulate the execution of a universal Turing machine without modification of model weights.",
        "author": "Schuurmans, Dale",
        "date": "2023-01-09",
        "doi": "10.48550/arXiv.2301.04589",
        "eprint": "2301.04589 [cs]",
        "eprinttype": "arxiv",
        "file": "Schuurmans - 2023 - Memory Augmented Large Language Models are Computationally Universal.pdf:/Users/endofstream/Zotero/storage/LKDCC3QE/Schuurmans - 2023 - Memory Augmented Large Language Models are Computationally Universal.pdf:application/pdf",
        "keywords": "evaluation:mathematical_proofs, supported_operations:insert, supported_operations:update, retrieval_data_type:text, retrieval_method:KV_search, updating_method:LLM-generated_KV_pairs, LLMs, computationally universal, external memory, finite automaton, Turing machine",
        "langid": "english",
        "number": "arXiv:2301.04589",
        "publisher": "arXiv",
        "title": "Memory Augmented Large Language Models are Computationally Universal",
        "type": "misc",
        "url": "http://arxiv.org/abs/2301.04589",
        "urldate": "2024-05-14",
        "year": "2023"
    },
    "wang_enhancing_2024": {
        "abstract": "Figure 3: Prompt for dialogue memory summarization.",
        "author": "Wang, Bing and Liang, Xinnian and Yang, Jian and Huang, Hui and Wu, Shuangzhi and Wu, Peihao and Lu, Lu and Ma, Zejun and Li, Zhoujun",
        "date": "2024-02-15",
        "doi": "10.48550/arXiv.2304.13343",
        "eprint": "2304.13343 [cs]",
        "eprinttype": "arxiv",
        "file": "Wang et al. - 2024 - Enhancing Large Language Model with Self-Controlled Memory Framework.pdf:/Users/endofstream/Zotero/storage/933B8I23/Wang et al. - 2024 - Enhancing Large Language Model with Self-Controlled Memory Framework.pdf:application/pdf",
        "keywords": "evaluation:quantitative, supported_operations:insert, supported_operations:delete, supported_operations:update, retrieval_data_type:text, retrieval_method:vector_similarity_search, updating_method:LLM-generated_sentences, LLMs, self-controlled memory, dialogue memory, memory summarization",
        "langid": "english",
        "number": "arXiv:2304.13343",
        "publisher": "arXiv",
        "title": "Enhancing Large Language Model with Self-Controlled Memory Framework",
        "type": "misc",
        "url": "http://arxiv.org/abs/2304.13343",
        "urldate": "2024-05-15",
        "year": "2024"
    },
    "zheng_memoryrepository_2024": {
        "abstract": "Since the release of ChatGPT, large language models (LLMs) have played a huge role in various industries. In the field of games, we have used LLMs to act as intelligent AI NPCs, making NPCs more intelligent. However, LLMs lack long-term memory and a human-like memory mechanism. We propose MemoryRepository, a memory mechanism for LLMs used in the AI NPC field. MemoryRepository enables the model to have short-term and long-term memory, inspired by human memory and forgetting mechanisms. This mechanism allows AI NPCs to forget and summarize past conversation records, providing long-term interaction capabilities and making NPCs more human-like. We created an example in which all NPCs are represented by LLMs adapted to MemoryRepository, showing that AI NPCs can conduct better long-term conversations and appear more human-like.",
        "author": "Zheng, Shijie and He, Keith and Yang, Le and Xiong, Jie",
        "date": "2024",
        "doi": "10.1109/ACCESS.2024.3393485",
        "file": "Zheng et al. - 2024 - MemoryRepository for AI NPC.pdf:/Users/endofstream/Zotero/storage/4I2RRUMC/Zheng et al. - 2024 - MemoryRepository for AI NPC.pdf:application/pdf",
        "issn": "2169-3536",
        "journaltitle": "IEEE Access",
        "keywords": "evaluation:quantitative, evaluation:mathematical_proofs, evaluation:case_study, supported_operations:insert, supported_operations:delete, retrieval_data_type:text, retrieval_method:vector_similarity_search, updating_method:LLM-generated_sentences, LLMs, AI NPC, long-term memory, human-like memory, conversation records, human-like interaction",
        "langid": "english",
        "pages": "62581--62596",
        "rights": "https://creativecommons.org/licenses/by-nc-nd/4.0/",
        "shortjournal": "IEEE Access",
        "title": "MemoryRepository for AI NPC",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/document/10508558/",
        "urldate": "2024-05-15",
        "volume": "12",
        "year": "2024"
    },
    "zhong_memorybank_2024": {
        "abstract": "Large Language Models (LLMs) have drastically reshaped our interactions with AI systems. Despite this, a notable hindrance remains\u2014the deficiency of a long-term memory mechanism within these models. We propose MemoryBank, a novel memory mechanism tailored for LLMs, enabling models to summon relevant memories, continually evolve through continuous memory updates, and comprehend and adapt to a user\u2019s personality over time. MemoryBank incorporates a memory updating mechanism inspired by the Ebbinghaus Forgetting Curve theory, offering a more human-like memory mechanism. We validate MemoryBank\u2019s effectiveness through the creation of an LLM-based chatbot named SiliconFriend in a long-term AI Companion scenario.",
        "author": "Zhong, Wanjun and Guo, Lianghong and Gao, Qiqi and Ye, He and Wang, Yanlin",
        "date": "2024-03-24",
        "doi": "10.1609/aaai.v38i17.29946",
        "file": "Zhong et al. - 2024 - MemoryBank Enhancing Large Language Models with Long-Term Memory.pdf:/Users/endofstream/Zotero/storage/GK3Z567F/Zhong et al. - 2024 - MemoryBank Enhancing Large Language Models with Long-Term Memory.pdf:application/pdf",
        "issn": "2374-3468, 2159-5399",
        "journaltitle": "Proceedings of the AAAI Conference on Artificial Intelligence",
        "keywords": "evaluation:quantitative, evaluation:qualitative, supported_operations:insert, supported_operations:delete, retrieval_data_type:text, retrieval_method:vector_similarity_search, updating_method:LLM-generated_sentences, LLMs, long-term memory, AI Companion, memory updating, Ebbinghaus Forgetting Curve",
        "langid": "english",
        "number": "17",
        "pages": "19724--19731",
        "shortjournal": "AAAI",
        "shorttitle": "MemoryBank",
        "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
        "type": "article",
        "url": "https://ojs.aaai.org/index.php/AAAI/article/view/29946",
        "urldate": "2024-03-30",
        "volume": "38",
        "year": "2024"
    }
};